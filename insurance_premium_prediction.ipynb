{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c9fa5-195a-4d1f-9eb1-2fb8b30685cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634b064-2444-4ca1-8bf2-fd9be65d19e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training data shape: (1200000, 21)\n",
      "Test data shape: (800000, 20)\n"
     ]
    }
   ],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Logarithmic Error\"\"\"\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57f90d-7c71-4da1-a0bb-1b260d4ade1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of training data:\n",
      "   id   Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
      "0   0  19.0  Female        10049.0        Married                   1.0   \n",
      "1   1  39.0  Female        31678.0       Divorced                   3.0   \n",
      "2   2  23.0    Male        25602.0       Divorced                   3.0   \n",
      "3   3  21.0    Male       141855.0        Married                   2.0   \n",
      "4   4  21.0    Male        39651.0         Single                   1.0   \n",
      "\n",
      "  Education Level     Occupation  Health Score  Location  ... Previous Claims  \\\n",
      "0      Bachelor's  Self-Employed     22.598761     Urban  ...             2.0   \n",
      "1        Master's            NaN     15.569731     Rural  ...             1.0   \n",
      "2     High School  Self-Employed     47.177549  Suburban  ...             1.0   \n",
      "3      Bachelor's            NaN     10.938144     Rural  ...             1.0   \n",
      "4      Bachelor's  Self-Employed     20.376094     Rural  ...             0.0   \n",
      "\n",
      "   Vehicle Age  Credit Score  Insurance Duration           Policy Start Date  \\\n",
      "0         17.0         372.0                 5.0  2023-12-23 15:21:39.134960   \n",
      "1         12.0         694.0                 2.0  2023-06-12 15:21:39.111551   \n",
      "2         14.0           NaN                 3.0  2023-09-30 15:21:39.221386   \n",
      "3          0.0         367.0                 1.0  2024-06-12 15:21:39.226954   \n",
      "4          8.0         598.0                 4.0  2021-12-01 15:21:39.252145   \n",
      "\n",
      "  Customer Feedback Smoking Status Exercise Frequency Property Type  \\\n",
      "0              Poor             No             Weekly         House   \n",
      "1           Average            Yes            Monthly         House   \n",
      "2              Good            Yes             Weekly         House   \n",
      "3              Poor            Yes              Daily     Apartment   \n",
      "4              Poor            Yes             Weekly         House   \n",
      "\n",
      "  Premium Amount  \n",
      "0         2869.0  \n",
      "1         1483.0  \n",
      "2          567.0  \n",
      "3          765.0  \n",
      "4         2022.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Missing values in training data:\n",
      "Age                      18705\n",
      "Annual Income            44949\n",
      "Marital Status           18529\n",
      "Number of Dependents    109672\n",
      "Occupation              358075\n",
      "Health Score             74076\n",
      "Previous Claims         364029\n",
      "Vehicle Age                  6\n",
      "Credit Score            137882\n",
      "Insurance Duration           1\n",
      "Customer Feedback        77824\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training data:\")\n",
    "missing_values = train_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cc190-07cc-4f10-a146-5fef01825e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying log transform to target variable\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Use log transform on target variable\n",
    "print(\"Applying log transform to target variable\")\n",
    "y_train_raw = train_data[\"Premium Amount\"].copy()\n",
    "train_data[\"Premium Amount\"] = np.log1p(train_data[\"Premium Amount\"])\n",
    "\n",
    "# Convert Policy Start Date to datetime and extract features\n",
    "train_data[\"Policy Start Date\"] = pd.to_datetime(\n",
    "    train_data[\"Policy Start Date\"], errors=\"coerce\"\n",
    ")\n",
    "test_data[\"Policy Start Date\"] = pd.to_datetime(\n",
    "    test_data[\"Policy Start Date\"], errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3024cbf-af2b-427f-8c81-876d5922086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced datetime features\n",
    "for df in [train_data, test_data]:\n",
    "    df[\"Policy_Start_Year\"] = df[\"Policy Start Date\"].dt.year\n",
    "    df[\"Policy_Start_Month\"] = df[\"Policy Start Date\"].dt.month\n",
    "    df[\"Policy_Start_Day\"] = df[\"Policy Start Date\"].dt.day\n",
    "    df[\"Policy_Start_Weekday\"] = df[\"Policy Start Date\"].dt.weekday\n",
    "    df[\"Policy_Start_Quarter\"] = df[\"Policy Start Date\"].dt.quarter\n",
    "    df[\"Policy_Start_DayOfYear\"] = df[\"Policy Start Date\"].dt.dayofyear\n",
    "    df[\"Policy_Start_IsWeekend\"] = df[\"Policy_Start_Weekday\"].isin([5, 6]).astype(int)\n",
    "    df[\"Policy_Start_IsMonthEnd\"] = df[\"Policy Start Date\"].dt.is_month_end.astype(int)\n",
    "    df[\"Policy_Start_IsMonthStart\"] = df[\"Policy Start Date\"].dt.is_month_start.astype(\n",
    "        int\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10519331-b194-41e2-b7fb-fd30acb1435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original datetime column\n",
    "train_data = train_data.drop(\"Policy Start Date\", axis=1)\n",
    "test_data = test_data.drop(\"Policy Start Date\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07c421-c081-40e3-9a6e-b71760b45c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering\n",
    "for df in [train_data, test_data]:\n",
    "    # Age-related features\n",
    "    df[\"Age_Squared\"] = df[\"Age\"] ** 2\n",
    "    df[\"Age_Cubed\"] = df[\"Age\"] ** 3\n",
    "    df[\"Log_Age\"] = np.log1p(df[\"Age\"])\n",
    "    df[\"Age_Bins\"] = pd.qcut(df[\"Age\"], q=5, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    # Income-related features\n",
    "    df[\"Log_Income\"] = np.log1p(df[\"Annual Income\"])\n",
    "    df[\"Income_Squared\"] = df[\"Annual Income\"] ** 2\n",
    "    df[\"Income_Bins\"] = pd.qcut(\n",
    "        df[\"Annual Income\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Health-related features\n",
    "    df[\"Health_Squared\"] = df[\"Health Score\"] ** 2\n",
    "    df[\"Health_Cubed\"] = df[\"Health Score\"] ** 3\n",
    "    df[\"Log_Health\"] = np.log1p(df[\"Health Score\"])\n",
    "    df[\"Health_Bins\"] = pd.qcut(\n",
    "        df[\"Health Score\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Credit-related features\n",
    "    df[\"Log_Credit\"] = np.log1p(df[\"Credit Score\"])\n",
    "    df[\"Credit_Squared\"] = df[\"Credit Score\"] ** 2\n",
    "    df[\"Credit_Bins\"] = pd.qcut(\n",
    "        df[\"Credit Score\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Duration-related features\n",
    "    df[\"Log_Duration\"] = np.log1p(df[\"Insurance Duration\"])\n",
    "    df[\"Duration_Squared\"] = df[\"Insurance Duration\"] ** 2\n",
    "    df[\"Duration_Bins\"] = pd.qcut(\n",
    "        df[\"Insurance Duration\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Claim-related features\n",
    "    df[\"Log_Claims\"] = np.log1p(df[\"Previous Claims\"])\n",
    "    df[\"Claims_Squared\"] = df[\"Previous Claims\"] ** 2\n",
    "    df[\"Claims_Bins\"] = pd.qcut(\n",
    "        df[\"Previous Claims\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Vehicle-related features\n",
    "    df[\"Log_Vehicle_Age\"] = np.log1p(df[\"Vehicle Age\"])\n",
    "    df[\"Vehicle_Age_Squared\"] = df[\"Vehicle Age\"] ** 2\n",
    "    df[\"Vehicle_Age_Bins\"] = pd.qcut(\n",
    "        df[\"Vehicle Age\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Dependent-related features\n",
    "    df[\"Log_Dependents\"] = np.log1p(df[\"Number of Dependents\"])\n",
    "    df[\"Dependents_Squared\"] = df[\"Number of Dependents\"] ** 2\n",
    "    df[\"Dependents_Bins\"] = pd.qcut(\n",
    "        df[\"Number of Dependents\"], q=5, labels=False, duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    # Advanced interaction features\n",
    "    df[\"Age_Income\"] = df[\"Age\"] * df[\"Annual Income\"]\n",
    "    df[\"Health_Credit\"] = df[\"Health Score\"] * df[\"Credit Score\"]\n",
    "    df[\"Age_Health\"] = df[\"Age\"] * df[\"Health Score\"]\n",
    "    df[\"Income_Dependents\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "    df[\"Age_Claims\"] = df[\"Age\"] * df[\"Previous Claims\"]\n",
    "    df[\"Duration_Claims\"] = df[\"Insurance Duration\"] * df[\"Previous Claims\"]\n",
    "    df[\"Health_Income\"] = df[\"Health Score\"] * df[\"Annual Income\"]\n",
    "    df[\"Credit_Duration\"] = df[\"Credit Score\"] * df[\"Insurance Duration\"]\n",
    "    df[\"Age_Duration\"] = df[\"Age\"] * df[\"Insurance Duration\"]\n",
    "    df[\"Claims_Dependents\"] = df[\"Previous Claims\"] * df[\"Number of Dependents\"]\n",
    "\n",
    "    # Ratio features\n",
    "    df[\"Income_Per_Dependent\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "    df[\"Health_Per_Age\"] = df[\"Health Score\"] / df[\"Age\"]\n",
    "    df[\"Credit_Per_Claim\"] = df[\"Credit Score\"] / (df[\"Previous Claims\"] + 1)\n",
    "    df[\"Duration_Per_Age\"] = df[\"Insurance Duration\"] / df[\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94bb8f-a3eb-43ac-82b6-865ffc742579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train = train_data.drop([\"Premium Amount\", \"id\"], axis=1)\n",
    "y_train = train_data[\"Premium Amount\"]  # Already log-transformed\n",
    "X_test = test_data.drop([\"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4fdc19-35c5-46f6-967e-e3a3826fded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5850d24-d170-496d-a2de-7207b959293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for numeric features\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[numeric_cols] = num_imputer.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = num_imputer.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36486f62-a665-4f40-a1d2-42ea75dc60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for categorical features\n",
    "for col in categorical_cols:\n",
    "    most_frequent = X_train[col].mode()[0]\n",
    "    X_train[col] = X_train[col].fillna(most_frequent)\n",
    "    X_test[col] = X_test[col].fillna(most_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4e98f-9784-4027-82c5-5d531d727d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Yeo-Johnson power transformation to numeric columns\n",
    "# This normalizes the data better than simple scaling\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "for col in numeric_cols:\n",
    "    if X_train[col].nunique() > 5:  # Only transform features with enough unique values\n",
    "        X_train[col] = power_transformer.fit_transform(\n",
    "            X_train[col].values.reshape(-1, 1)\n",
    "        )\n",
    "        X_test[col] = power_transformer.transform(X_test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e751ea-6d05-4f66-89ef-6aa41896ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical to numeric using one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc11e9-c116-46f9-8551-5f43b8e6216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_train and X_test have the same columns\n",
    "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b146e8c-9941-4576-8766-5aad5bfa9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns in test that are not in train with zeros\n",
    "missing_cols = set(X_test.columns) - set(X_train.columns)\n",
    "for col in missing_cols:\n",
    "    X_train[col] = 0\n",
    "\n",
    "# Ensure column order is the same\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# Split the data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e68357-f2ec-4247-963b-74a589acdb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LightGBM models...\n"
     ]
    }
   ],
   "source": [
    "# Define base models\n",
    "print(\"\\nTraining LightGBM models...\")\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"num_iterations\": 1000,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 0.1,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c34317-8022-4ac7-b87e-409c6b85d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lightgbm datasets\n",
    "train_dataset = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "val_dataset = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# Train model with cross-validation\n",
    "n_splits = 3  # Reduced number of folds for stability\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88c8e6-fa57-4ce8-80b1-e58fa2de4dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1/3\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 1.04826\tvalid_1's rmse: 1.05409\n",
      "Fold 1 RMSLE: 1.0541\n",
      "\n",
      "Training fold 2/3\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[941]\ttraining's rmse: 1.04994\tvalid_1's rmse: 1.05144\n",
      "Fold 2 RMSLE: 1.0514\n",
      "\n",
      "Training fold 3/3\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\ttraining's rmse: 1.04942\tvalid_1's rmse: 1.05161\n",
      "Fold 3 RMSLE: 1.0516\n",
      "\n",
      "Mean CV RMSLE: 1.0524 (+/- 0.0012)\n"
     ]
    }
   ],
   "source": [
    "# Train model with cross-validation\n",
    "n_splits = 3  # Reduced number of folds for stability\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train), 1):\n",
    "    print(f\"\\nTraining fold {fold}/{n_splits}\")\n",
    "    X_train_fold = X_train.iloc[train_idx]\n",
    "    y_train_fold = y_train.iloc[train_idx]\n",
    "    X_val_fold = X_train.iloc[val_idx]\n",
    "    y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "    train_dataset_fold = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "    val_dataset_fold = lgb.Dataset(X_val_fold, label=y_val_fold)\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_dataset_fold,\n",
    "        valid_sets=[train_dataset_fold, val_dataset_fold],\n",
    "        num_boost_round=500,  # Reduced number of rounds\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=30)\n",
    "        ],  # Reduced early stopping rounds\n",
    "    )\n",
    "\n",
    "    models.append(model)\n",
    "\n",
    "    # Evaluate on validation fold\n",
    "    val_pred = model.predict(X_val_fold)\n",
    "    val_pred_original = np.expm1(val_pred)\n",
    "    y_val_original = np.expm1(y_val_fold)\n",
    "    fold_rmsle = rmsle(y_val_original, val_pred_original)\n",
    "    cv_scores.append(fold_rmsle)\n",
    "    print(f\"Fold {fold} RMSLE: {fold_rmsle:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV RMSLE: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaafb61-7508-45b0-8913-dec572ce9964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model on all data...\n"
     ]
    }
   ],
   "source": [
    "# Train final model on all training data\n",
    "print(\"\\nTraining final model on all data...\")\n",
    "train_dataset_full = lgb.Dataset(X_train, label=y_train)\n",
    "final_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_dataset_full,\n",
    "    num_boost_round=500,  # Removed early stopping callback for final model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb2d38-8f0a-46d3-82bc-ac1853eb7d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "              Feature     Importance\n",
      "6        Credit_Score  894481.607517\n",
      "1       Annual_Income  836947.415612\n",
      "21         Log_Income  684719.235975\n",
      "4     Previous_Claims  486021.107862\n",
      "3        Health_Score  421879.785043\n",
      "49      Health_Income  396290.263812\n",
      "8   Policy_Start_Year  280278.014265\n",
      "34         Log_Claims  141923.901343\n",
      "24     Health_Squared   84592.611558\n",
      "28         Log_Credit   81729.551333\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame(\n",
    "    {\n",
    "        \"Feature\": final_model.feature_name(),\n",
    "        \"Importance\": final_model.feature_importance(importance_type=\"gain\"),\n",
    "    }\n",
    ")\n",
    "feature_importance = feature_importance.sort_values(\"Importance\", ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
